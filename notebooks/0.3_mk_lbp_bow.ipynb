{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LICENSE             descriptors.pkl     \u001b[1m\u001b[36mnotebooks\u001b[m\u001b[m           \u001b[1m\u001b[36mskin_lesion_cad\u001b[m\u001b[m\n",
      "README.md           descriptors_all.pkl \u001b[1m\u001b[36mreferences\u001b[m\u001b[m\n",
      "bovw.pkl            \u001b[1m\u001b[36mdocs\u001b[m\u001b[m                \u001b[1m\u001b[36mreports\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                \u001b[1m\u001b[36mmodels\u001b[m\u001b[m              requirements.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from skin_lesion_cad.data.BOVW import DenseDescriptor, BagofWords, LBPDescriptor\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_chall2_class(path):\n",
    "    if \"bcc\" in str(path):\n",
    "        return \"bcc\"\n",
    "    elif \"mel\" in str(path):\n",
    "        return \"mel\"\n",
    "    elif \"scc\" in str(path):\n",
    "        return \"scc\"\n",
    "    else:\n",
    "        raise ValueError(\"class needs to be bcc, mel or scc\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# SAMPLE_SIZE = 1000\n",
    "\n",
    "chall = \"chall2\"\n",
    "train_path = Path(f\"data/processed/{chall}/train\")\n",
    "training_names = train_path.rglob(\"*_inpaint_0_5*\")\n",
    "\n",
    "# Get path to all images and save them in a list\n",
    "image_paths = [i for i in training_names]\n",
    "\n",
    "\n",
    "# Currently only sampling few images for quick testing\n",
    "# image_paths = [i for i in training_names]\n",
    "if chall==\"chall1\":\n",
    "    image_classes = [0 if (\"nevus\" in str(i)) else 1 for i in image_paths]\n",
    "elif chall==\"chall2\":\n",
    "    image_classes = [get_chall2_class(str(i)) for i in image_paths]\n",
    "mask_paths = [image_path.parent/\n",
    "              Path(image_path.name.replace(\"inpaint\",\"mask\")) for image_path in image_paths]\n",
    "\n",
    "\n",
    "# BRISK is a good replacement to SIFT. ORB also works but didn;t work well for this example\n",
    "\n",
    "brisk = cv2.BRISK_create(thresh=30, octaves=0)\n",
    "dense_brisk = DenseDescriptor(\n",
    "    descriptor=brisk, min_keypoints=100, max_keypoints=500, kp_size=25)\n",
    "lbp = LBPDescriptor(descriptor=brisk, min_keypoints=100, max_keypoints=500, kp_size=25)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "image_paths[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PosixPath('data/processed/chall2/train/bcc/bcc01703_inpaint_0_5.png')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def _load_and_extract_des(image_path, mask_path, descriptor):\n",
    "    im = cv2.imread(str(image_path))\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    mask = cv2.imread(str(mask_path))\n",
    "    kpts, des = descriptor.detectAndCompute(im_gray, mask)\n",
    "    return des\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    des_list = Parallel(verbose=10)(\n",
    "        delayed(_load_and_extract_des)(filename, mask_paths[i], lbp) for i, filename in enumerate(image_paths)\n",
    "    )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2505 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2649 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2949 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3026 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3105 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3429 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3682 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3769 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3945 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4125 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4216 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4309 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4402 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4689 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4786 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5082 out of 5082 | elapsed:  7.4min finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "bovw = BagofWords(n_words=10, n_jobs=-1, random_state=None)\n",
    "classifier = SVC(max_iter=10000, probability=True, class_weight='balanced', C=1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    des_list, image_classes, test_size=0.33, random_state=42)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_train_preprocessed = bovw.fit_transform(X_train, y_train)\n",
    "X_test_preprocessed = bovw.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "classifier.fit(X_train_preprocessed, y_train)\n",
    "y_pred = classifier.predict(X_test_preprocessed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.53      0.57       689\n",
      "           1       0.68      0.59      0.63       871\n",
      "           2       0.10      0.26      0.14       118\n",
      "\n",
      "    accuracy                           0.54      1678\n",
      "   macro avg       0.46      0.46      0.45      1678\n",
      "weighted avg       0.61      0.54      0.57      1678\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_cl = XGBClassifier()\n",
    "\n",
    "xgb_cl.fit(X_train_preprocessed, y_train)\n",
    "y_pred = xgb_cl.predict(X_test_preprocessed)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57       689\n",
      "           1       0.64      0.77      0.70       871\n",
      "           2       0.17      0.03      0.04       118\n",
      "\n",
      "    accuracy                           0.62      1678\n",
      "   macro avg       0.47      0.44      0.44      1678\n",
      "weighted avg       0.59      0.62      0.60      1678\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Whole image lbp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "\n",
    "def lbph(image, n_points_radius=[(24, 8), (8, 3), (12, 3), (8, 2), (8, 1)], method=\"default\", eps=1e-7):\n",
    "    hist_concat = np.array([])\n",
    "    for (n_points, radius) in n_points_radius:\n",
    "\n",
    "        lbp = local_binary_pattern(\n",
    "            image, n_points, radius, method)\n",
    "        (hist, _) = np.histogram(lbp.ravel(),\n",
    "                                bins=np.arange(0, n_points + 3),\n",
    "                                range=(0, n_points + 2))\n",
    "        # normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    "        hist_concat = np.append(hist_concat,hist)\n",
    "    return hist_concat\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_lbphfeat(image_path):\n",
    "    im = cv2.imread(str(image_path))\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return lbph(im_gray)\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    lbp_feats = Parallel(verbose=10)(\n",
    "        delayed(get_lbphfeat)(image_path) for image_path in image_paths\n",
    "    )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2505 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2649 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2949 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3026 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3105 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3429 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3682 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3769 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3945 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4125 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4216 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4309 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4402 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4689 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4786 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5082 out of 5082 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "pd.DataFrame(lbp_feats, columns=[\"lbp\"+str(i) for i in range(len(lbp_feats[0]))])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp0</th>\n",
       "      <th>lbp1</th>\n",
       "      <th>lbp2</th>\n",
       "      <th>lbp3</th>\n",
       "      <th>lbp4</th>\n",
       "      <th>lbp5</th>\n",
       "      <th>lbp6</th>\n",
       "      <th>lbp7</th>\n",
       "      <th>lbp8</th>\n",
       "      <th>lbp9</th>\n",
       "      <th>...</th>\n",
       "      <th>lbp60</th>\n",
       "      <th>lbp61</th>\n",
       "      <th>lbp62</th>\n",
       "      <th>lbp63</th>\n",
       "      <th>lbp64</th>\n",
       "      <th>lbp65</th>\n",
       "      <th>lbp66</th>\n",
       "      <th>lbp67</th>\n",
       "      <th>lbp68</th>\n",
       "      <th>lbp69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766444</td>\n",
       "      <td>0.049145</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.027220</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339532</td>\n",
       "      <td>0.148314</td>\n",
       "      <td>0.010666</td>\n",
       "      <td>0.049626</td>\n",
       "      <td>0.117162</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>0.247082</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.078392</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>0.025461</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288031</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>0.152964</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.283532</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.789949</td>\n",
       "      <td>0.037473</td>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>0.024063</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>0.018925</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244932</td>\n",
       "      <td>0.128501</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.119765</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>0.040144</td>\n",
       "      <td>0.384903</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>0.019935</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396464</td>\n",
       "      <td>0.141456</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.128664</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.004077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756760</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.020228</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>0.144746</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.142001</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>0.043486</td>\n",
       "      <td>0.433006</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>0.765340</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.014337</td>\n",
       "      <td>0.014222</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364760</td>\n",
       "      <td>0.137591</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.141354</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>0.041698</td>\n",
       "      <td>0.216016</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>0.754593</td>\n",
       "      <td>0.045308</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.010427</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290369</td>\n",
       "      <td>0.145851</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.137639</td>\n",
       "      <td>0.028370</td>\n",
       "      <td>0.045808</td>\n",
       "      <td>0.295648</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.001387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>0.774743</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.058846</td>\n",
       "      <td>0.157425</td>\n",
       "      <td>0.025793</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.002471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>0.751670</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.021251</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442473</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>0.063086</td>\n",
       "      <td>0.132792</td>\n",
       "      <td>0.020987</td>\n",
       "      <td>0.058963</td>\n",
       "      <td>0.135415</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>0.004997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>0.781604</td>\n",
       "      <td>0.051427</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269328</td>\n",
       "      <td>0.194725</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.151192</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.304462</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5082 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lbp0      lbp1      lbp2      lbp3      lbp4      lbp5      lbp6  \\\n",
       "0     0.766444  0.049145  0.022204  0.018023  0.027220  0.003809  0.012728   \n",
       "1     0.688889  0.078392  0.020324  0.025461  0.028476  0.009269  0.016527   \n",
       "2     0.789949  0.037473  0.013410  0.017797  0.024063  0.002256  0.015415   \n",
       "3     0.760518  0.056828  0.014369  0.019935  0.019288  0.006084  0.010744   \n",
       "4     0.756760  0.038118  0.011893  0.019313  0.020939  0.002948  0.013011   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5077  0.765340  0.061475  0.014337  0.014222  0.025691  0.003670  0.010552   \n",
       "5078  0.754593  0.045308  0.014772  0.020233  0.021971  0.003600  0.014772   \n",
       "5079  0.774743  0.033183  0.011927  0.013754  0.020198  0.002789  0.008175   \n",
       "5080  0.751670  0.023072  0.010322  0.013965  0.021251  0.003643  0.010322   \n",
       "5081  0.781604  0.051427  0.012460  0.013140  0.017444  0.005664  0.005664   \n",
       "\n",
       "          lbp7      lbp8      lbp9  ...     lbp60     lbp61     lbp62  \\\n",
       "0     0.011334  0.013006  0.001579  ...  0.339532  0.148314  0.010666   \n",
       "1     0.024344  0.012507  0.002792  ...  0.288031  0.191872  0.002620   \n",
       "2     0.018925  0.010026  0.000877  ...  0.244932  0.128501  0.005868   \n",
       "3     0.022136  0.012298  0.001165  ...  0.396464  0.141456  0.012125   \n",
       "4     0.020228  0.015654  0.001830  ...  0.155842  0.144746  0.002858   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5077  0.010552  0.012272  0.001032  ...  0.364760  0.137591  0.007838   \n",
       "5078  0.032026  0.010427  0.001241  ...  0.290369  0.145851  0.007626   \n",
       "5079  0.013754  0.016255  0.001250  ...  0.345316  0.111167  0.009846   \n",
       "5080  0.019429  0.015179  0.002429  ...  0.442473  0.106433  0.017739   \n",
       "5081  0.016765  0.008382  0.000680  ...  0.269328  0.194725  0.002439   \n",
       "\n",
       "         lbp63     lbp64     lbp65     lbp66     lbp67     lbp68     lbp69  \n",
       "0     0.049626  0.117162  0.028335  0.046849  0.247082  0.008855  0.003582  \n",
       "1     0.021358  0.152964  0.034113  0.021951  0.283532  0.002818  0.000742  \n",
       "2     0.039077  0.119765  0.028341  0.040144  0.384903  0.006735  0.001734  \n",
       "3     0.042138  0.128664  0.034688  0.043263  0.187004  0.010122  0.004077  \n",
       "4     0.045279  0.142001  0.029588  0.043486  0.433006  0.002410  0.000785  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5077  0.043938  0.141354  0.036548  0.041698  0.216016  0.007301  0.002956  \n",
       "5078  0.041222  0.137639  0.028370  0.045808  0.295648  0.006079  0.001387  \n",
       "5079  0.058846  0.157425  0.025793  0.057379  0.222488  0.009267  0.002471  \n",
       "5080  0.063086  0.132792  0.020987  0.058963  0.135415  0.017114  0.004997  \n",
       "5081  0.021270  0.151192  0.036624  0.016799  0.304462  0.002393  0.000768  \n",
       "\n",
       "[5082 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    lbp_feats, image_classes, test_size=0.3, random_state=42)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "svc = SVC(kernel='rbf', probability=True, class_weight='balanced', C=1.0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.57      0.60       617\n",
      "           1       0.72      0.64      0.68       801\n",
      "           2       0.16      0.36      0.22       107\n",
      "\n",
      "    accuracy                           0.59      1525\n",
      "   macro avg       0.50      0.53      0.50      1525\n",
      "weighted avg       0.64      0.59      0.61      1525\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_cl = XGBClassifier()\n",
    "\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "y_pred = xgb_cl.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.59      0.62       617\n",
      "           1       0.68      0.81      0.74       801\n",
      "           2       0.43      0.03      0.05       107\n",
      "\n",
      "    accuracy                           0.67      1525\n",
      "   macro avg       0.58      0.48      0.47      1525\n",
      "weighted avg       0.65      0.67      0.64      1525\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(lbp_feats)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.766444</td>\n",
       "      <td>0.049145</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.027220</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339532</td>\n",
       "      <td>0.148314</td>\n",
       "      <td>0.010666</td>\n",
       "      <td>0.049626</td>\n",
       "      <td>0.117162</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>0.247082</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.078392</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>0.025461</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288031</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>0.152964</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.283532</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.789949</td>\n",
       "      <td>0.037473</td>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>0.024063</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>0.018925</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244932</td>\n",
       "      <td>0.128501</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.119765</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>0.040144</td>\n",
       "      <td>0.384903</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>0.019935</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396464</td>\n",
       "      <td>0.141456</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.128664</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.004077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756760</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.020228</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>0.144746</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.142001</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>0.043486</td>\n",
       "      <td>0.433006</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>0.765340</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.014337</td>\n",
       "      <td>0.014222</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364760</td>\n",
       "      <td>0.137591</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.141354</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>0.041698</td>\n",
       "      <td>0.216016</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>0.754593</td>\n",
       "      <td>0.045308</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.010427</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290369</td>\n",
       "      <td>0.145851</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.137639</td>\n",
       "      <td>0.028370</td>\n",
       "      <td>0.045808</td>\n",
       "      <td>0.295648</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.001387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>0.774743</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.111167</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.058846</td>\n",
       "      <td>0.157425</td>\n",
       "      <td>0.025793</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.002471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>0.751670</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.021251</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442473</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>0.063086</td>\n",
       "      <td>0.132792</td>\n",
       "      <td>0.020987</td>\n",
       "      <td>0.058963</td>\n",
       "      <td>0.135415</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>0.004997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>0.781604</td>\n",
       "      <td>0.051427</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269328</td>\n",
       "      <td>0.194725</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.151192</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.304462</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5082 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.766444  0.049145  0.022204  0.018023  0.027220  0.003809  0.012728   \n",
       "1     0.688889  0.078392  0.020324  0.025461  0.028476  0.009269  0.016527   \n",
       "2     0.789949  0.037473  0.013410  0.017797  0.024063  0.002256  0.015415   \n",
       "3     0.760518  0.056828  0.014369  0.019935  0.019288  0.006084  0.010744   \n",
       "4     0.756760  0.038118  0.011893  0.019313  0.020939  0.002948  0.013011   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5077  0.765340  0.061475  0.014337  0.014222  0.025691  0.003670  0.010552   \n",
       "5078  0.754593  0.045308  0.014772  0.020233  0.021971  0.003600  0.014772   \n",
       "5079  0.774743  0.033183  0.011927  0.013754  0.020198  0.002789  0.008175   \n",
       "5080  0.751670  0.023072  0.010322  0.013965  0.021251  0.003643  0.010322   \n",
       "5081  0.781604  0.051427  0.012460  0.013140  0.017444  0.005664  0.005664   \n",
       "\n",
       "            7         8         9   ...        60        61        62  \\\n",
       "0     0.011334  0.013006  0.001579  ...  0.339532  0.148314  0.010666   \n",
       "1     0.024344  0.012507  0.002792  ...  0.288031  0.191872  0.002620   \n",
       "2     0.018925  0.010026  0.000877  ...  0.244932  0.128501  0.005868   \n",
       "3     0.022136  0.012298  0.001165  ...  0.396464  0.141456  0.012125   \n",
       "4     0.020228  0.015654  0.001830  ...  0.155842  0.144746  0.002858   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5077  0.010552  0.012272  0.001032  ...  0.364760  0.137591  0.007838   \n",
       "5078  0.032026  0.010427  0.001241  ...  0.290369  0.145851  0.007626   \n",
       "5079  0.013754  0.016255  0.001250  ...  0.345316  0.111167  0.009846   \n",
       "5080  0.019429  0.015179  0.002429  ...  0.442473  0.106433  0.017739   \n",
       "5081  0.016765  0.008382  0.000680  ...  0.269328  0.194725  0.002439   \n",
       "\n",
       "            63        64        65        66        67        68        69  \n",
       "0     0.049626  0.117162  0.028335  0.046849  0.247082  0.008855  0.003582  \n",
       "1     0.021358  0.152964  0.034113  0.021951  0.283532  0.002818  0.000742  \n",
       "2     0.039077  0.119765  0.028341  0.040144  0.384903  0.006735  0.001734  \n",
       "3     0.042138  0.128664  0.034688  0.043263  0.187004  0.010122  0.004077  \n",
       "4     0.045279  0.142001  0.029588  0.043486  0.433006  0.002410  0.000785  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5077  0.043938  0.141354  0.036548  0.041698  0.216016  0.007301  0.002956  \n",
       "5078  0.041222  0.137639  0.028370  0.045808  0.295648  0.006079  0.001387  \n",
       "5079  0.058846  0.157425  0.025793  0.057379  0.222488  0.009267  0.002471  \n",
       "5080  0.063086  0.132792  0.020987  0.058963  0.135415  0.017114  0.004997  \n",
       "5081  0.021270  0.151192  0.036624  0.016799  0.304462  0.002393  0.000768  \n",
       "\n",
       "[5082 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.13 64-bit ('cad': conda)"
  },
  "interpreter": {
   "hash": "49574378497f446692c7e26f7d0f985f921d43351aeee8284e547a417bd9147b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}